# HydroLLMs

**Can Large Language Models Effectively Reason about Adverse Weather Conditions?**

Welcome to **HydroLLMs**, a repository dedicated to sharing the **research findings, codebase, and evaluation** framework from our comprehensive study on the reasoning capabilities of Large Language Models (LLMs) when classifying adverse weather events using real-world disaster reports.

This repository provides a **complete and transparent workflow**—from data acquisition and preprocessing to fine-tuning and evaluation of multiple LLM architectures. It also includes practical insights into optimizing training efficiency and model stability, aiming to serve as a reference for **researchers, developers, and disaster management** practitioners interested in applying LLMs to text-based hazard analytics.

## Table of Contents


- [Overview](#overview)
- [Key Findings](#key-findings)
- [Dataset](#dataset)
- [Models Evaluated](#models-evaluated)
- [Results](#results)
- [Usage](#usage)
- [Lessons Learned](#lessons-learned)
- [Citation](#citation)

## Overview

HydroLLMs investigates the performance of seven transformer-based LLMs—including **BART, BERT, LLaMA-2, and LLaMA-3** models—on multi-label classification of **over 19 years of National Weather Service (NWS)** flood reports from Charleston County, South Carolina. The study focuses on three critical disaster categories: 

Flood, 

Thunderstorm, 

and Cyclonic.

![Figure 1](Results/Figure-1.jpg)  
